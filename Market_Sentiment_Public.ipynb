{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 0: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from pytube import extract\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from youtubesearchpython import *\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: Define all the different functions we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will allow us to calculate the AGE of a youtube video (how long ago a video was uploaded).\n",
    "def time_string_to_days(time_str):\n",
    "    # Define regular expression patterns to extract days, months, and years\n",
    "    seconds_pattern = r'(\\d+)\\s*second'\n",
    "    minutes_pattern = r'(\\d+)\\s*minute'\n",
    "    hours_pattern = r'(\\d+)\\s*hour'\n",
    "    days_pattern = r'(\\d+)\\s*day'\n",
    "    months_pattern = r'(\\d+)\\s*month'\n",
    "    years_pattern = r'(\\d+)\\s*year'\n",
    "    # Find days, months, and years in the time string\n",
    "    seconds = sum(int(match.group(1)) for match in re.finditer(seconds_pattern, time_str))\n",
    "    minutes = sum(int(match.group(1)) for match in re.finditer(minutes_pattern, time_str))\n",
    "    hours = sum(int(match.group(1)) for match in re.finditer(hours_pattern, time_str))\n",
    "    days = sum(int(match.group(1)) for match in re.finditer(days_pattern, time_str))\n",
    "    months = sum(int(match.group(1)) for match in re.finditer(months_pattern, time_str))\n",
    "    years = sum(int(match.group(1)) for match in re.finditer(years_pattern, time_str))\n",
    "    # Calculate total days\n",
    "    total_days = seconds / 84600 + minutes / 1440 + hours / 24 + days + months * 30 + years * 365\n",
    "    return total_days\n",
    "\n",
    "# This function will allow us to look, for each video transcript, at only the relevant parts (containing our keywords).\n",
    "# For each sentence is the raw_list, we look if there is a keyword (contained in kw_list). If there is, we store that\n",
    "# sentence, the previous and the next one in a new list (result_list) so that we can analyse it later. If the sentence\n",
    "# that contains the keyword happens to be after a sentence that already contained a keyword, the two shall be joined\n",
    "# together as most likely they are part of the same overall phrase.\n",
    "def join_sentences_with_keyword(raw_list, kw_list):\n",
    "    result_list = []\n",
    "    check_list = []\n",
    "    i = 0\n",
    "    n = len(raw_list)\n",
    "    while i < n-1:\n",
    "        if raw_list[i] in check_list:\n",
    "            s = len(result_list)-1\n",
    "            result_list[s] = result_list[s] + raw_list[i+1]\n",
    "            i = i+1\n",
    "        else:\n",
    "            if i == 0:\n",
    "                if any(keyword in raw_list[i] for keyword in kw_list):\n",
    "                    joined_sentences = raw_list[i] + raw_list[i+1]\n",
    "                    result_list.append(joined_sentences)\n",
    "                    check_list.append(raw_list[i+1])\n",
    "                    i = i+1\n",
    "                else:\n",
    "                    i = i+1\n",
    "            elif n-1 > i > 0:\n",
    "                if any(keyword in raw_list[i] for keyword in kw_list):\n",
    "                    joined_sentences = raw_list[i-1] + raw_list[i] + raw_list[i + 1]\n",
    "                    result_list.append(joined_sentences)\n",
    "                    check_list.append(raw_list[i+1])\n",
    "                    i = i+1\n",
    "                else:\n",
    "                    i = i+1\n",
    "            else:\n",
    "                break\n",
    "    return result_list\n",
    "\n",
    "# This function derives the synonyms of a word\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wn.synsets(word):\n",
    "        synonyms.extend(synset.lemma_names())\n",
    "    # Remove duplicates\n",
    "    synonyms = list(set(synonyms))\n",
    "    # For each word, also add the version with the first letter capitalized\n",
    "    synonyms_with_capitalized = [s.capitalize() for s in synonyms]\n",
    "    # For each word, substitute the \"_\" with a \" \"\n",
    "    word_list = list()\n",
    "    for word in synonyms_with_capitalized:\n",
    "        word_clean = word.replace(\"_\", \" \")\n",
    "        word_list.append(word_clean)\n",
    "    # synonyms.extend(word_list)\n",
    "    # Add a space before and after the word so that in the transcript we take only these words and not also the words\n",
    "    # containing these letters\n",
    "    word_list_final = [\" \" + s + \" \" for s in word_list]\n",
    "    return word_list_final\n",
    "\n",
    "# This function cleans the transcript of a youtube video from all non-verbal phrases\n",
    "def clean(transcript_list):\n",
    "    # Regular expression to match words inside square brackets\n",
    "    bracket_pattern = r'\\[.*?\\]'\n",
    "    # Regular expression to match dots\n",
    "    dot_pattern = r'\\.'\n",
    "    comma_pattern = r','\n",
    "    # Let's now clean the transcript\n",
    "    cleaned_transcript_list = []\n",
    "    # Iterate through each dictionary in the list\n",
    "    for item in transcript_list:\n",
    "        # Check if the 'text' value contains words inside square brackets\n",
    "        if re.search(bracket_pattern, item['text']):\n",
    "            # If it does, do not include this dictionary in the cleaned list\n",
    "            continue\n",
    "        else:\n",
    "            # Otherwise, remove dots and commas from the 'text' value\n",
    "            cleaned_text = re.sub(dot_pattern, '', item['text'])\n",
    "            cleaned_text = re.sub(comma_pattern, '', cleaned_text)\n",
    "            item['text'] = cleaned_text\n",
    "            cleaned_transcript_list.append(item)\n",
    "    return cleaned_transcript_list\n",
    "\n",
    "# This function extrapolates the video IDs from their urls\n",
    "def video_id(value):\n",
    "    query = urlparse.urlparse(value)\n",
    "    if query.hostname == 'youtu.be':\n",
    "        return query.path[1:]\n",
    "    if query.hostname in ('www.youtube.com', 'youtube.com'):\n",
    "        if query.path == '/watch':\n",
    "            p = urlparse.parse_qs(query.query)\n",
    "            return p['v'][0]\n",
    "        if query.path[:7] == '/embed/':\n",
    "            return query.path.split('/')[2]\n",
    "        if query.path[:3] == '/v/':\n",
    "            return query.path.split('/')[2]\n",
    "    # fail?\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: Define your research interest\n",
    "\n",
    "The inputs of this section are three: (i) the period of time; (ii) the keywords that one wants to analyse; (iii) the youtube channels that will be used as data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the keywords, create a list of relevant words we want to look for in the videos\n",
    "word = input(\"Insert the topic you are interested in: \")\n",
    "synonyms = get_synonyms(word)\n",
    "print(\"Synonyms for\", word + \":\", synonyms)\n",
    "\n",
    "# Ask the user if they want to add other synonyms\n",
    "while True:\n",
    "    choice = input(\"Do you want to add more synonyms? (yes/no): \")\n",
    "    if choice.lower() == 'yes':\n",
    "        new_synonym = input(\"Enter a synonym: \")\n",
    "        synonyms.append(new_synonym)\n",
    "        synonyms.append(new_synonym.capitalize())\n",
    "    elif choice.lower() == 'no':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 'yes' or 'no'.\")\n",
    "\n",
    "kw_list = synonyms\n",
    "\n",
    "# Define a function that from these calculates the number of days to which we want to go back in our analysis.\n",
    "# Indicate the time period of interest\n",
    "start_date_years = int(input('In which year (01/01/xxxx) do you want the analysis to start? '))\n",
    "end_date_years = int(input('In which year (31/12/xxxx) do you want the analysis to end? : '))\n",
    "today = datetime.now()\n",
    "start_period_date = (today - datetime(start_date_years, 1, 1)).days\n",
    "end_period_date = (datetime(end_date_years, 12, 31) - today).days\n",
    "\n",
    "# Indicate the channel IDs of the channels you want to analyse: remember, try to get the \"main\" points of view in the\n",
    "# industries/businesses you are interested in.\n",
    "# Also, consider choosing older/newer channels based on the tme period you are interested in analysing. For instance,\n",
    "# if I want my analysis to run from 2000, I need to choose channels that we born before 2000s, otherwise we will not be\n",
    "# able to do so.\n",
    "channel_id_1 = input('Insert the ID of the channels you are interested in analysing: ')\n",
    "channel_id_2 = input('Insert the ID of the channels you are interested in analysing: ')\n",
    "playlist_1 = Playlist(playlist_from_channel_id(channel_id_1))\n",
    "playlist_2 = Playlist(playlist_from_channel_id(channel_id_2))\n",
    "\n",
    "# Let's import as many videos as necessary in order to reach the starting date\n",
    "# For playlist 1\n",
    "start_date_tracker1 = 0\n",
    "while playlist_1.hasMoreVideos:\n",
    "    if start_date_tracker1 < start_period_date:\n",
    "        playlist_1.getNextVideos() # We get more videos for the playlist\n",
    "        playlist_1_videos = playlist_1.videos # We start to isolate the last element of the playlist to understand at\n",
    "        # which date it was uploaded at\n",
    "        s = len(playlist_1_videos)-1\n",
    "        video_s = playlist_1_videos[s]\n",
    "        intermediate1_s = video_s.get('accessibility', None)\n",
    "        Intermediate2_s = intermediate1_s.get('title', None)\n",
    "        pattern = r'views(.*)$'  # Define the regular expression pattern: all the durations are in this format\n",
    "        match = re.search(pattern, Intermediate2_s)  # Find the matching part of the string\n",
    "        if match:\n",
    "            period_str = match.group(1).strip()\n",
    "            total_days = time_string_to_days(period_str)  # Convert the period str into a total number of days\n",
    "            start_date_tracker1 = total_days\n",
    "            print(f'Videos Retrieved for Channel 1: {len(playlist_1.videos)}')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# For playlist 2\n",
    "start_date_tracker2 = 0\n",
    "while playlist_2.hasMoreVideos:\n",
    "    if start_date_tracker2 < start_period_date:\n",
    "        playlist_2.getNextVideos()\n",
    "        playlist_2_videos = playlist_2.videos\n",
    "        s = len(playlist_2_videos)-1\n",
    "        video_s = playlist_2_videos[s]\n",
    "        intermediate1_s = video_s.get('accessibility', None)\n",
    "        Intermediate2_s = intermediate1_s.get('title', None)\n",
    "        pattern = r'views(.*)$'  # Define the regular expression pattern: all the durations are in this format\n",
    "        match = re.search(pattern, Intermediate2_s)  # Find the matching part of the string\n",
    "        if match:\n",
    "            period_str = match.group(1).strip()\n",
    "            total_days = time_string_to_days(period_str)  # Convert the period str into a total number of days\n",
    "            start_date_tracker2 = total_days\n",
    "            print(f'Videos Retrieved for Channel 2: {len(playlist_2.videos)}')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# First, let's extract the links and the durations of these playlists.\n",
    "# We will use the durations to (i) give an information to the user of the total length of videos analysed and (ii) to\n",
    "# create an unbiased analysis.\n",
    "\n",
    "# Playlist 1\n",
    "i = 0\n",
    "n = len(playlist_1.videos)\n",
    "times1 = list()\n",
    "links1 = list()\n",
    "while i < n:\n",
    "    video_i = playlist_1.videos[i]\n",
    "    times1.append(video_i.get('duration', None))\n",
    "    links1.append(video_i.get('link', None))\n",
    "    i = i+1\n",
    "\n",
    "# Playlist 2\n",
    "i = 0\n",
    "n = len(playlist_2.videos)\n",
    "times2 = list()\n",
    "links2 = list()\n",
    "while i < n:\n",
    "    video_i = playlist_2.videos[i]\n",
    "    times2.append(video_i.get('duration', None))\n",
    "    links2.append(video_i.get('link', None))\n",
    "    i = i+1\n",
    "\n",
    "# NB: This process will eliminate some videos even though they might be later analysed. This means the total duration\n",
    "# analysed is just an approximation (an underestimation, more precisely) of the real duration analysed. I left the code\n",
    "# in this way because re-adapting the code would have taken me too much time and this is not a critical feature. More\n",
    "# importantly, some key information might be contained in those videos, and having an error in the total duration\n",
    "# is not worth excluding that information.\n",
    "\n",
    "# Now that we have the total times of the videos, we can build an \"unbiased\" point of view.\n",
    "\n",
    "# Playlist 1\n",
    "playlist1videos_raw = playlist_1.videos\n",
    "i = len(playlist1videos_raw)-1\n",
    "periods1 = list()\n",
    "while i >= 0:\n",
    "    video_i = playlist1videos_raw[i]\n",
    "    intermediate1_i = video_i.get('accessibility', None)\n",
    "    Intermediate2_i = intermediate1_i.get('title', None)\n",
    "    pattern = r'views(.*)$' # Define the regular expression pattern: all the durations are in this format\n",
    "    match = re.search(pattern, Intermediate2_i) # Find the matching part of the string\n",
    "    if match:\n",
    "        period_str = match.group(1).strip()\n",
    "        total_days = time_string_to_days(period_str) # Convert the period str into a total number of days\n",
    "        periods1.append(total_days)\n",
    "        i = i-1\n",
    "\n",
    "# Now that we have the period of time when the videos were uploaded, I will delete the \"diff\" number of videos from the\n",
    "# end. This is because there are two elements to consider in the \"time period\": (i) the start, (ii) the end. For (i),\n",
    "# we already took it into account when downloading the videos --> we want to download them as far as we can until the\n",
    "# beginning of the starting year. For (ii) we will do it now, using this method I explained above.\n",
    "filtered_dates1 = [date for date in periods1 if date > end_period_date]\n",
    "diff = len(playlist1videos_raw)-len(filtered_dates1)\n",
    "del links1[:diff]\n",
    "# Now let's do the same for the times and dates lists\n",
    "del times1[:diff]\n",
    "del periods1[:diff]\n",
    "\n",
    "# Now let's calculate the total times for playlist 1.\n",
    "i = 0\n",
    "n = len(times1)\n",
    "times_clean1 = list()\n",
    "total_seconds1 = 0\n",
    "while i < n: # This simply cleans the list from all the 'None' elements.\n",
    "    if times1[i] == None:\n",
    "        i = i+1\n",
    "    else:\n",
    "        times_clean1.append(str(times1[i]))\n",
    "        i = i+1\n",
    "i = 0\n",
    "n = len(times_clean1)\n",
    "total_seconds1 = 0\n",
    "while i < n:\n",
    "    time_elements = times_clean1[i].split(':')\n",
    "    hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "    minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "    seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "    total_seconds1 = total_seconds1 + hours * 3600 + minutes * 60 + seconds\n",
    "    i = i+1\n",
    "\n",
    "# Playlist 2\n",
    "playlist2videos_raw = playlist_2.videos\n",
    "i = len(playlist2videos_raw)-1\n",
    "periods2 = list()\n",
    "while i >= 0:\n",
    "    video_i = playlist2videos_raw[i]\n",
    "    intermediate1_i = video_i.get('accessibility', None)\n",
    "    Intermediate2_i = intermediate1_i.get('title', None)\n",
    "    pattern = r'views(.*)$' # Define the regular expression pattern: all the durations are in this format\n",
    "    match = re.search(pattern, Intermediate2_i) # Find the matching part of the string\n",
    "    if match:\n",
    "        period_str = match.group(1).strip()\n",
    "        total_days = time_string_to_days(period_str) # Convert the period str into a total number of days\n",
    "        periods2.append(total_days)\n",
    "        i = i-1\n",
    "\n",
    "filtered_dates2 = [date for date in periods2 if date > end_period_date]\n",
    "diff = len(playlist2videos_raw)-len(filtered_dates2)\n",
    "del links2[:diff]\n",
    "# Now let's do the same for the times\n",
    "del times2[:diff]\n",
    "del periods2[:diff]\n",
    "\n",
    "# Now let's calculate the total times for playlist 2.\n",
    "i = 0\n",
    "n = len(times2)\n",
    "times_clean2 = list()\n",
    "total_seconds2 = 0\n",
    "while i < n: # This simply cleans the list from all the 'None' elements.\n",
    "    if times2[i] == None:\n",
    "        i = i+1\n",
    "    else:\n",
    "        times_clean2.append(str(times2[i]))\n",
    "        i = i+1\n",
    "i = 0\n",
    "n = len(times_clean2)\n",
    "total_seconds2 = 0\n",
    "while i < n:\n",
    "    time_elements = times_clean2[i].split(':')\n",
    "    hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "    minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "    seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "    total_seconds2 = total_seconds2 + hours * 3600 + minutes * 60 + seconds\n",
    "    i = i+1\n",
    "\n",
    "# Show the use the total time of the videos if we were not to adjust it\n",
    "print('The total raw duration of the videos would be: ')\n",
    "print('For playlist one: ', int(total_seconds1/3600), ' hours')\n",
    "print('For playlist two: ', int(total_seconds2/3600), ' hours')\n",
    "\n",
    "# Now let's make the two playlist the same total time duration, so that we are not subject to biases.\n",
    "if total_seconds2 > total_seconds1:\n",
    "    while total_seconds2 // total_seconds1 > 1:\n",
    "        step_size = int(total_seconds2/total_seconds1)\n",
    "        del links2[::step_size]\n",
    "        del periods2[::step_size]\n",
    "        del times_clean2[::step_size]\n",
    "        i = 0 # This simply calculates total_seconds1\n",
    "        n = len(times_clean1)\n",
    "        total_seconds1 = 0\n",
    "        while i < n:\n",
    "            time_elements = times_clean1[i].split(':')\n",
    "            hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "            minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "            seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "            total_seconds1 = total_seconds1 + hours * 3600 + minutes * 60 + seconds\n",
    "            i = i + 1\n",
    "        i = 0 # This simply calculates total_seconds2\n",
    "        n = len(times_clean2)\n",
    "        total_seconds2 = 0\n",
    "        while i < n:\n",
    "            time_elements = times_clean2[i].split(':')\n",
    "            hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "            minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "            seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "            total_seconds2 = total_seconds2 + hours * 3600 + minutes * 60 + seconds\n",
    "            i = i + 1\n",
    "else:\n",
    "    while total_seconds1 // total_seconds2 > 1:\n",
    "        step_size = int(total_seconds1 / total_seconds2)\n",
    "        del links1[::step_size]\n",
    "        del periods1[::step_size]\n",
    "        del times_clean1[::step_size]\n",
    "        i = 0\n",
    "        n = len(times_clean1)\n",
    "        total_seconds1 = 0\n",
    "        while i < n:\n",
    "            time_elements = times_clean1[i].split(':')\n",
    "            hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "            minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "            seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "            total_seconds1 = total_seconds1 + hours * 3600 + minutes * 60 + seconds\n",
    "            i = i + 1\n",
    "        i = 0\n",
    "        n = len(times_clean2)\n",
    "        total_seconds2 = 0\n",
    "        while i < n:\n",
    "            time_elements = times_clean2[i].split(':')\n",
    "            hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "            minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "            seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "            total_seconds2 = total_seconds2 + hours * 3600 + minutes * 60 + seconds\n",
    "            i = i + 1\n",
    "\n",
    "# Now let's calculate again the total time of the videos that we are going to analyse:\n",
    "i = 0\n",
    "n = len(times_clean1)\n",
    "total_seconds1 = 0\n",
    "while i < n:\n",
    "    time_elements = times_clean1[i].split(':')\n",
    "    hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "    minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "    seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "    total_seconds1 = total_seconds1 + hours * 3600 + minutes * 60 + seconds\n",
    "    i = i+1\n",
    "i = 0\n",
    "n = len(times_clean2)\n",
    "total_seconds2 = 0\n",
    "while i < n:\n",
    "    time_elements = times_clean2[i].split(':')\n",
    "    hours = int(float(time_elements[0])) if len(time_elements) >= 1 else 0\n",
    "    minutes = int(float(time_elements[1])) if len(time_elements) >= 2 else 0\n",
    "    seconds = int(float(time_elements[2])) if len(time_elements) >= 3 else 0\n",
    "    total_seconds2 = total_seconds2 + hours * 3600 + minutes * 60 + seconds\n",
    "    i = i+1\n",
    "\n",
    "# Show the use the total time of videos that will be actually analysed\n",
    "total_time = total_seconds1 + total_seconds2\n",
    "print('The total time of the videos that will be analysed is: ', int(total_time/3600), ' hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Retrieving and Analyzing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a list where we will store our results\n",
    "Sent_Overall_Score = list()\n",
    "Sent_Pol_Score = list()\n",
    "Sent_Subj_Score = list()\n",
    "\n",
    "# First, for each link in the playlists we want to create the video ID.\n",
    "IDs_1 = list()\n",
    "IDs_2 = list()\n",
    "for url in links1:\n",
    "    IDs_1.append(extract.video_id(url))\n",
    "for url in links2:\n",
    "    IDs_2.append(extract.video_id(url))\n",
    "\n",
    "print(len(IDs_1))\n",
    "print(len(periods1))\n",
    "print(len(IDs_2))\n",
    "print(len(periods2))\n",
    "\n",
    "# FOR PLAYLIST 1 -------------------------------------------------------------------------------------------------------\n",
    "# Then, from the raw YT transcript we want to create a list of phrases.\n",
    "n_obs1 = list()\n",
    "Sent_Overall_Score = list()\n",
    "Sent_Subj_Score = list()\n",
    "Sent_Pol_Score = list()\n",
    "r = len(IDs_1)-1\n",
    "while r >= 0:\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(IDs_1[r])\n",
    "        cleaned_transcript = clean(transcript)\n",
    "# To distinguish the timings of the speech, we are going to use duration of the phrase.\n",
    "# More specifically, we are going to use the duration of the phrase divided by its number of characters as a proxy of\n",
    "# whether there is a comma or a dot after that phrase. (HP1: phrases do not interrupt mid-subtitle -> I tested this\n",
    "# empirically and it is true approx 95% of the time)\n",
    "# I will not use the duration indicated in the video as it is not accurate (I empirically tested it and does not\n",
    "# represent at all the reality of timings). I will derive it from the starts\n",
    "\n",
    "# First, extract the starting times of each phrase\n",
    "        start = [line['start'] for line in cleaned_transcript]\n",
    "        duration = [line['duration'] for line in cleaned_transcript]\n",
    "        end = [x + y for x, y in zip(start, duration)]\n",
    "        i = 0\n",
    "        n = len(end) - 2\n",
    "        diff = list()\n",
    "        while i < n:\n",
    "            diff.append(start[i + 1] - end[i])\n",
    "            i = i + 1\n",
    "# Then, calculate the duration of each phrase, the number of characters and the key descriptive statistics we are going\n",
    "# to use in the subsequent calculations\n",
    "        dur = list()\n",
    "        n = len(start)-1\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            dur.append(start[i+1]-start[i])\n",
    "            i = i+1\n",
    "\n",
    "# Since dur has one element less than the transcript, to make them both of the appropriate lenght I shall remove the\n",
    "# last element of the transcript (assuming it does not contain any essential information, that to me seems plausible)\n",
    "        n = len(cleaned_transcript)-1\n",
    "        del cleaned_transcript[n]\n",
    "\n",
    "# Now calculate the duration per character, our key metric for inserting commas and dots in the text, and the\n",
    "# relative statistics we will use to distinguish where to put dots or commas.\n",
    "\n",
    "# First, we create a list with the length of each world\n",
    "        words = [line['text'] for line in cleaned_transcript]\n",
    "        words_length = list()\n",
    "        for word in words:\n",
    "            words_length.append(len(word))\n",
    "\n",
    "# Then we calculate the average duration per character of each word\n",
    "        dur_per_char = list()\n",
    "        n = len(words_length)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            dur_per_char.append(dur[i]/words_length[i])\n",
    "            i = i+1\n",
    "\n",
    "# Finally, we calculate keys statistics we are going to use later\n",
    "        comma_dur = np.quantile(dur_per_char, 0.65)\n",
    "        dot_dur = np.quantile(dur_per_char, 0.90)\n",
    "\n",
    "# Now create a loop that adds either commas or dots based on these metrics. In this part of the code, although I\n",
    "# left it so that one can add also dots and try to reason on them, I will only use commas for simplicity.\n",
    "        cleaned_text = [line['text'] for line in cleaned_transcript]\n",
    "\n",
    "        n = len(cleaned_text)-1\n",
    "        i=0\n",
    "        while i < n:\n",
    "            if diff[i] > 0:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')\n",
    "                i = i + 1\n",
    "            elif dot_dur > dur_per_char[i] > comma_dur:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')\n",
    "                i = i + 1\n",
    "            elif dur_per_char[i] > dot_dur:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')  # I have substituted commas here as well\n",
    "                i = i + 1\n",
    "            else:\n",
    "                i = i + 1\n",
    "\n",
    "# First, join all the elements of the new list with the correct punctuation.\n",
    "        cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "# Create a list with each phrase as element\n",
    "        raw_list = cleaned_text.split(',')\n",
    "\n",
    "# Remove empty strings caused by consecutive periods\n",
    "        raw_list = [sentence.strip() for sentence in raw_list if sentence.strip()]\n",
    "\n",
    "# Use the function to create a list of relevant sentences\n",
    "        filtered_text = join_sentences_with_keyword(raw_list, kw_list)\n",
    "\n",
    "# Then analyse your data\n",
    "# First, calculate polarity and subjectivity scores for each \"filtered\" sentence\n",
    "        polarity_scores = list()\n",
    "        subjectivity_scores = list()\n",
    "        for sentence in filtered_text:\n",
    "            analyse_text = TextBlob(sentence)\n",
    "            polar_score=analyse_text.sentiment.polarity\n",
    "            subj_score=analyse_text.sentiment.subjectivity\n",
    "            Sent_Pol_Score.append(polar_score)\n",
    "            Sent_Subj_Score.append(subj_score)\n",
    "            Sent_Overall_Score.append(polar_score*subj_score)\n",
    "        print(\"Element \", r, \" processed correctly\")\n",
    "# Store both the information about how many observations were made at each period1 element in another list, so\n",
    "# that we can reconciliate the observations with the period in which they were made\n",
    "        n_obs1.append(len(filtered_text))\n",
    "        r = r-1\n",
    "# If there is an error, we will skip that video and go on to the next one\n",
    "    except Exception:\n",
    "        del periods1[r]\n",
    "        print(\"Error in processing the \", r, \"th element\")\n",
    "        r = r-1\n",
    "\n",
    "# Print and store results\n",
    "print(len(Sent_Overall_Score))\n",
    "print(len(periods1))\n",
    "print(n_obs1)\n",
    "print(Sent_Overall_Score)\n",
    "print(Sent_Pol_Score)\n",
    "print(Sent_Subj_Score)\n",
    "print(periods1)\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Overall_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Pol_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Subj_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for period in periods1:\n",
    "        writer.write(str(period) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for obs in n_obs1:\n",
    "        writer.write(str(obs) + \",\")\n",
    "\n",
    "print('PROCESS ENDED CORRECTLY for playlist 1')\n",
    "\n",
    "# FOR PLAYLIST 2 -------------------------------------------------------------------------------------------------------\n",
    "# Then, from the raw YT transcript we want to create a list of phrases.\n",
    "n_obs2 = list()\n",
    "Sent_Overall_Score = list()\n",
    "Sent_Subj_Score = list()\n",
    "Sent_Pol_Score = list()\n",
    "r = len(IDs_2)-1\n",
    "while r >= 0:\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(IDs_2[r])\n",
    "        cleaned_transcript = clean(transcript)\n",
    "\n",
    "# Following the same logics as before:\n",
    "# First, extract the key timings of each phrase\n",
    "        start = [line['start'] for line in cleaned_transcript]\n",
    "        duration = [line['duration'] for line in cleaned_transcript]\n",
    "        end = [x + y for x, y in zip(start, duration)]\n",
    "        i = 0\n",
    "        n = len(end) - 2\n",
    "        diff = list()\n",
    "        while i < n:\n",
    "            diff.append(start[i + 1] - end[i])\n",
    "            i = i + 1\n",
    "# Then, calculate the duration of each phrase, the number of characters and the key descriptive statistics we are going\n",
    "# to use in the subsequent calculations\n",
    "        dur = list()\n",
    "        n = len(start)-1\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            dur.append(start[i+1]-start[i])\n",
    "            i = i+1\n",
    "\n",
    "# Since dur has one element less than the transcript, to make them both of the appropriate length I shall remove the\n",
    "# last element of the transcript (assuming it does not contain any essential information, that to me seems plausible)\n",
    "        n = len(cleaned_transcript)-1\n",
    "        del cleaned_transcript[n]\n",
    "\n",
    "# Now calculate the duration per character, our key metric for inserting commas and dots in the text, and the\n",
    "# relative statistics we will use to distinguish where to put dots or commas.\n",
    "\n",
    "# First, we create a list with the length of each world\n",
    "        words = [line['text'] for line in cleaned_transcript]\n",
    "        words_length = list()\n",
    "        for word in words:\n",
    "            words_length.append(len(word))\n",
    "\n",
    "# Then we calculate the average duration per character of each word\n",
    "        dur_per_char = list()\n",
    "        n = len(words_length)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            dur_per_char.append(dur[i]/words_length[i])\n",
    "            i = i+1\n",
    "\n",
    "# Finally, we calculate keys statistics we are going to use later\n",
    "        comma_dur = np.quantile(dur_per_char, 0.65)\n",
    "        dot_dur = np.quantile(dur_per_char, 0.90)\n",
    "\n",
    "# Now create a loop that adds either commas or dots based on these metrics. In this part of the code, although I\n",
    "# left it so that one can add also dots and try to reason on them, I will only use commas for simplicity.\n",
    "        cleaned_text = [line['text'] for line in cleaned_transcript]\n",
    "\n",
    "        n = len(cleaned_text)-1\n",
    "        i=0\n",
    "        while i < n:\n",
    "            if diff[i] > 0:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')\n",
    "                i = i + 1\n",
    "            elif dot_dur > dur_per_char[i] > comma_dur:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')\n",
    "                i = i + 1\n",
    "            elif dur_per_char[i] > dot_dur:\n",
    "                cleaned_text[i] = str(cleaned_text[i] + ',')  # I have substituted commas here as well\n",
    "                i = i + 1\n",
    "            else:\n",
    "                i = i + 1\n",
    "\n",
    "# First, join all the elements of the new list with the correct punctuation.\n",
    "        cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "# Create a list with each phrase as element\n",
    "        raw_list = cleaned_text.split(',')\n",
    "\n",
    "# Remove empty strings caused by consecutive periods\n",
    "        raw_list = [sentence.strip() for sentence in raw_list if sentence.strip()]\n",
    "\n",
    "# Use the function to create a list of relevant sentences\n",
    "        filtered_text = join_sentences_with_keyword(raw_list, kw_list)\n",
    "\n",
    "# Then analyse your data\n",
    "# First, calculate polarity and subjectivity scores for each \"filtered\" sentence\n",
    "        polarity_scores = list()\n",
    "        subjectivity_scores = list()\n",
    "        for sentence in filtered_text:\n",
    "            analyse_text = TextBlob(sentence)\n",
    "            polar_score=analyse_text.sentiment.polarity\n",
    "            subj_score=analyse_text.sentiment.subjectivity\n",
    "            Sent_Pol_Score.append(polar_score)\n",
    "            Sent_Subj_Score.append(subj_score)\n",
    "            Sent_Overall_Score.append(polar_score*subj_score)\n",
    "        print(\"Element \", r, \" processed correctly\")\n",
    "        # Store both the information about how many observations were made at each period1 element in another list, so\n",
    "        # that we can reconciliate the observations with the period in which they were made\n",
    "        n_obs2.append(len(filtered_text))\n",
    "        r = r-1\n",
    "# If there is an error, we will skip that video and go on to the next one\n",
    "    except Exception:\n",
    "        del periods2[r]\n",
    "        print(\"Error in processing the \", r, \"th element\")\n",
    "        r = r-1\n",
    "\n",
    "# Store results\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Overall_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Pol_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for score in Sent_Subj_Score:\n",
    "        writer.write(str(score) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for period in periods2:\n",
    "        writer.write(str(period) + \",\")\n",
    "with open(r'aaaa', 'a') as writer:\n",
    "    for obs in n_obs2:\n",
    "        writer.write(str(obs) + \",\")\n",
    "\n",
    "print('PROCESS ENDED CORRECTLY for playlist 2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
